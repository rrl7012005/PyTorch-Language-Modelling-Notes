{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP3JEgm2knluScvVxktkAjJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrl7012005/PyTorch-Language-Modelling-Notes/blob/main/GPT_detailed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Can use the following to download files from the internet, if you want to use the shakespeare file\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "id": "n_V-z2vT6Re9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLLWcm4Nd9BI",
        "outputId": "f8cd2a9e-1a9f-4228-ca56-62f501403927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "232309\n",
            "﻿DOROTHY AND THE WIZARD IN OZ\n",
            "\n",
            "  BY\n",
            "\n",
            "  L. FRANK BAUM\n",
            "\n",
            "  AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ, OZMA OF OZ, ETC.\n",
            "\n",
            "  ILLUSTRATED BY JOHN R. NEILL\n",
            "\n",
            "  BOOKS OF WONDER WILLIAM MORROW & CO., INC. NEW Y\n",
            "\n",
            " !\"&'()*,-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyz﻿\n",
            "81\n"
          ]
        }
      ],
      "source": [
        "chars = \"\"\n",
        "with open(\"wizard_of_oz.txt\", \"r\", encoding='utf-8') as f:\n",
        "  text = f.read() #read the file as a string\n",
        "\n",
        "print(len(text))\n",
        "print(text[:200])\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"\".join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizing"
      ],
      "metadata": {
        "id": "B-7jKGOIgll-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tokenizer consists of an encoder and a decoder, where each character can be encoded into an id (different from embedding). The following is a character level tokenizer, we could have a word level tokenizer. If we use a word level tokenizer we have a very large vocabulary, but there are less elements in the dataset to encode and decode. For our character level tokenizer, there are more elements in dataset to encode and decode but the vocab size is smaller. Typically use sub word units so you're something in between.\n",
        "\n",
        "For tokenization, can use tiktoken open ai library (for words) or sentence piece library for (sub word units)"
      ],
      "metadata": {
        "id": "REVBWda5gmxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string_to_int = {ch:i for i, ch in enumerate(chars)} #Create mapping\n",
        "int_to_string = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda x: [string_to_int[c] for c in x]\n",
        "decode = lambda x: \"\".join([int_to_string[i] for i in x])\n",
        "\n",
        "print(encode(\"hello\"))\n",
        "print(decode(encode(\"hello\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNLDz81Rg07n",
        "outputId": "26a7c379-98b5-42f0-e0d1-e0696dc9fdb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[61, 58, 65, 65, 68]\n",
            "hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Its best to efficient with our data and strings aren't. Also we should use a ML framework so use pytorch"
      ],
      "metadata": {
        "id": "gwMEPvbJhv-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ivJOAmVh3eS",
        "outputId": "2abd3202-89d7-42ba-d6d6-6c87b20927b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "tensor([80, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,  1, 47,\n",
            "        33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26, 49,  0,\n",
            "         0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,  0,  0,\n",
            "         1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1, 47, 33,\n",
            "        50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1, 36, 25,\n",
            "        38, 28,  1, 39, 30,  1, 39, 50,  9,  1, 39, 50, 37, 25,  1, 39, 30,  1,\n",
            "        39, 50,  9,  1, 29, 44, 27, 11,  0,  0,  1,  1, 33, 36, 36, 45, 43, 44,\n",
            "        42, 25, 44, 29, 28,  1, 26, 49,  1, 34, 39, 32, 38,  1, 42, 11,  1, 38,\n",
            "        29, 33, 36, 36,  0,  0,  1,  1, 26, 39, 39, 35, 43,  1, 39, 30,  1, 47,\n",
            "        39, 38, 28, 29, 42,  1, 47, 33, 36, 36, 33, 25, 37,  1, 37, 39, 42, 42,\n",
            "        39, 47,  1,  4,  1, 27, 39, 11,  9,  1, 33, 38, 27, 11,  1, 38, 29, 47,\n",
            "         1, 49])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "block_size = 8\n",
        "batch_size = 4"
      ],
      "metadata": {
        "id": "jAPYwJrejQ1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device) #load data on the gpu\n",
        "  return x, y\n",
        "\n",
        "x, y = get_batch('train')\n",
        "print(x.shape, y.shape)\n",
        "print(x)\n",
        "print(y)\n",
        "\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = x[b, :t+1]\n",
        "    target = y[b, t]\n",
        "    print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBvT57E2vqtr",
        "outputId": "6360ebd3-8d63-4cbe-8684-14c5d290161e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 8]) torch.Size([4, 8])\n",
            "tensor([[58, 66,  1, 62, 67, 73, 68,  1],\n",
            "        [67, 57,  1, 73, 61, 58,  1, 44],\n",
            "        [57,  1, 73, 61, 58,  1, 73, 76],\n",
            "        [61, 68, 71, 72, 58,  1, 54, 67]], device='cuda:0')\n",
            "tensor([[66,  1, 62, 67, 73, 68,  1, 73],\n",
            "        [57,  1, 73, 61, 58,  1, 44, 62],\n",
            "        [ 1, 73, 61, 58,  1, 73, 76, 68],\n",
            "        [68, 71, 72, 58,  1, 54, 67, 57]], device='cuda:0')\n",
            "when input is [58] the target: 66\n",
            "when input is [58, 66] the target: 1\n",
            "when input is [58, 66, 1] the target: 62\n",
            "when input is [58, 66, 1, 62] the target: 67\n",
            "when input is [58, 66, 1, 62, 67] the target: 73\n",
            "when input is [58, 66, 1, 62, 67, 73] the target: 68\n",
            "when input is [58, 66, 1, 62, 67, 73, 68] the target: 1\n",
            "when input is [58, 66, 1, 62, 67, 73, 68, 1] the target: 73\n",
            "when input is [67] the target: 57\n",
            "when input is [67, 57] the target: 1\n",
            "when input is [67, 57, 1] the target: 73\n",
            "when input is [67, 57, 1, 73] the target: 61\n",
            "when input is [67, 57, 1, 73, 61] the target: 58\n",
            "when input is [67, 57, 1, 73, 61, 58] the target: 1\n",
            "when input is [67, 57, 1, 73, 61, 58, 1] the target: 44\n",
            "when input is [67, 57, 1, 73, 61, 58, 1, 44] the target: 62\n",
            "when input is [57] the target: 1\n",
            "when input is [57, 1] the target: 73\n",
            "when input is [57, 1, 73] the target: 61\n",
            "when input is [57, 1, 73, 61] the target: 58\n",
            "when input is [57, 1, 73, 61, 58] the target: 1\n",
            "when input is [57, 1, 73, 61, 58, 1] the target: 73\n",
            "when input is [57, 1, 73, 61, 58, 1, 73] the target: 76\n",
            "when input is [57, 1, 73, 61, 58, 1, 73, 76] the target: 68\n",
            "when input is [61] the target: 68\n",
            "when input is [61, 68] the target: 71\n",
            "when input is [61, 68, 71] the target: 72\n",
            "when input is [61, 68, 71, 72] the target: 58\n",
            "when input is [61, 68, 71, 72, 58] the target: 1\n",
            "when input is [61, 68, 71, 72, 58, 1] the target: 54\n",
            "when input is [61, 68, 71, 72, 58, 1, 54] the target: 67\n",
            "when input is [61, 68, 71, 72, 58, 1, 54, 67] the target: 57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We only train the transformer from a chunk of the dataset which we shuffle and choose randomly. We train the transformer on all 8 samples here (if block size is 8) so for one sequence of text we get 8 different data."
      ],
      "metadata": {
        "id": "4ujDg_CX62Qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is {context} the target is {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYeNRFbniT5W",
        "outputId": "4d2c089d-1396-4bb3-9f05-1c7bd5b97b21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([80]) the target is 28\n",
            "when input is tensor([80, 28]) the target is 39\n",
            "when input is tensor([80, 28, 39]) the target is 42\n",
            "when input is tensor([80, 28, 39, 42]) the target is 39\n",
            "when input is tensor([80, 28, 39, 42, 39]) the target is 44\n",
            "when input is tensor([80, 28, 39, 42, 39, 44]) the target is 32\n",
            "when input is tensor([80, 28, 39, 42, 39, 44, 32]) the target is 49\n",
            "when input is tensor([80, 28, 39, 42, 39, 44, 32, 49]) the target is 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our GPU, we stack a bunch of blocks and the number of blocks is our batch_size. Its how many things we're doing in parallel in our GPU. Btw numpy works on the cpu and torch, cuda works on gpu. CPU is better for large complex operations but GPU is better for many more but smaller operations."
      ],
      "metadata": {
        "id": "IM0qbmEMjsUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PyTorch stuff"
      ],
      "metadata": {
        "id": "0RLwokPvkOBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "randint = torch.randint(-100, 100, (6,))\n",
        "randint"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF6nZtrhkPfy",
        "outputId": "338b4f78-9b74-41c2-dfa1-7a5a76b33b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ -5,  53, -85, -90, -66, -10])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.tensor([[0.1, 1.2], [2.2, 3.1]])\n",
        "tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v9DX_R2kZJM",
        "outputId": "468fe340-3082-480e-a241-bbbb5e698285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1000, 1.2000],\n",
              "        [2.2000, 3.1000]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zeros = torch.zeros(2, 3) #shape is argument, dtype is float\n",
        "ones = torch.ones(3, 4)\n",
        "\n",
        "zeros, ones"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwXp0DKzkh3z",
        "outputId": "c3dbf8e9-5830-47d6-ad7b-185ef845140f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0., 0.],\n",
              "         [0., 0., 0.]]),\n",
              " tensor([[1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.empty(2, 3)\n",
        "input #creates a tensor of shape specificied with uninitialized values\n",
        "#it can be faster to use this since it doesn't initialize if you want to create a\n",
        "#a random tensor instead of using zeros or ones\n",
        "input.fill_(1.0) #fill all elements with 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4LDc81TkrLx",
        "outputId": "e9495bd9-8a72-40fd-b824-f37e78cdc576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arange = torch.arange(5) #similar to range in python\n",
        "linspace = torch.linspace(3, 10, steps=5) #similar to linspace in python\n",
        "arange, linspace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6GXMM0slOh6",
        "outputId": "34afcdfe-11c0-4982-efd0-a25703b626a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0, 1, 2, 3, 4]),\n",
              " tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#logarithmically creates something check it out\n",
        "logspace = torch.logspace(start=-10, end=10, steps=5)\n",
        "logspace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7zIurEJle08",
        "outputId": "a22f6818-6f6c-4edd-f493-1b92e49df2c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000e-10, 1.0000e-05, 1.0000e+00, 1.0000e+05, 1.0000e+10])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eye = torch.eye(5) #dimension of identity\n",
        "eye"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YeoilX_luvB",
        "outputId": "1d9950f6-7b2b-4fff-91ae-7f77f7eda48a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Can also do like function (ones_like, zeros_like, empty_like)\n",
        "#Can also specify datatype while decarling tensor\n",
        "a = torch.empty((2, 3), dtype=torch.int64)\n",
        "empty_like = torch.empty_like(a)\n",
        "empty_like"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf4oDsh8l8re",
        "outputId": "757d9b59-ba63-4ccc-c892-e4e461657e8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[              0,  98036351716544,  98036310022928],\n",
              "        [135021587812640,               0,               0]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firsy argument is range, second is shape. rand function generates float32, randint gives integer"
      ],
      "metadata": {
        "id": "CtvxN5xXu0UX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "int_64 = torch.randint(1, (3,2), dtype=torch.int64)\n",
        "float_32 = torch.rand(2, 3)\n",
        "int_64, float_32\n",
        "\n",
        "#cast via the following\n",
        "casted = int_64.float()\n",
        "casted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSmaBnA3uxG1",
        "outputId": "07d8d45f-1f74-4eaa-b2e1-124391297c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Switch to GPU and compare the following\n",
        "\n",
        "import time #or do %%time for the whole cell\n",
        "start_time = time.time()\n",
        "\n",
        "#operations here\n",
        "\n",
        "torch_rand1 = torch.rand(10000, 10000).to(device)\n",
        "torch_rand2 = torch.rand(10000, 10000).to(device)\n",
        "torch_rand = (torch_rand1 @ torch_rand2)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"{elapsed_time:.8f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8ymAVyJmc-0",
        "outputId": "50ce5777-2103-44c5-8dd0-6b4cba7a9b90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.24842238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More TORCH functions"
      ],
      "metadata": {
        "id": "t0xU9IxqoGPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Draw from a discrete probability distribution using torch.multinomial (place the probabilities in a tensor) and the samples returned will be the indices of the probability tensor."
      ],
      "metadata": {
        "id": "fmEjVKf6oQAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities = torch.tensor([0.2, 0.2, 0.2, 0.2, 0.2])\n",
        "samples = torch.multinomial(probabilities, num_samples=10, replacement=True)\n",
        "print(samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aX1of-BDoJYF",
        "outputId": "e066dd18-0291-4b68-8d6f-f425b7eedce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 2, 0, 4, 1, 3, 2, 4, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cat function concatenates tensors. Brackets around the 2 tensors you want to concatenate and along which dimension"
      ],
      "metadata": {
        "id": "ZuthCysmorL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.tensor([1, 2, 3, 4])\n",
        "out = torch.cat((tensor, torch.tensor([5])), dim=0)\n",
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_VieL3GotRQ",
        "outputId": "e291e128-c3b5-48e4-ea0c-6b95ae0c56ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Torch.tril returns the lower triangular part of the argument and torch.triu gives the upper triangular part of the argument."
      ],
      "metadata": {
        "id": "LAjF1rzmpAP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lower = torch.tril(torch.ones(5, 5))\n",
        "upper = torch.triu(torch.ones(5, 5))\n",
        "\n",
        "lower, upper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VX3gEuZpG2n",
        "outputId": "2963bc2d-16f9-4764-8600-937b5fc4bf3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1., 0., 0., 0., 0.],\n",
              "         [1., 1., 0., 0., 0.],\n",
              "         [1., 1., 1., 0., 0.],\n",
              "         [1., 1., 1., 1., 0.],\n",
              "         [1., 1., 1., 1., 1.]]),\n",
              " tensor([[1., 1., 1., 1., 1.],\n",
              "         [0., 1., 1., 1., 1.],\n",
              "         [0., 0., 1., 1., 1.],\n",
              "         [0., 0., 0., 1., 1.],\n",
              "         [0., 0., 0., 0., 1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "any pytorch tensor has masked fill method which is like numpy, used for whenever something is 0 send it to float -ing"
      ],
      "metadata": {
        "id": "xoGPo7kCpADU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.zeros(5, 5)\n",
        "y = x.masked_fill(torch.tril(torch.ones(5, 5)) == 0, float('-inf'))"
      ],
      "metadata": {
        "id": "ZS7p21MMper0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.exp(y) #returns e^x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O95pw9Rrp4FJ",
        "outputId": "62800c64-f25b-43fe-da2b-4d3838d952fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transpose and stack"
      ],
      "metadata": {
        "id": "uiNxlLizo_s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros(2, 3, 4).transpose(0, 2) #the 2 arguments inside transpose are the dimensions to swap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxmpMZ83qC2a",
        "outputId": "4e0a699e-19e0-4f86-ecfc-1f86c19ba08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]],\n",
              "\n",
              "        [[0., 0.],\n",
              "         [0., 0.],\n",
              "         [0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1 = torch.tensor([1, 2, 3])\n",
        "tensor2 = torch.tensor([4, 5, 6])\n",
        "tensor3 = torch.tensor([7, 8, 9])\n",
        "\n",
        "stacked_tensor = torch.stack([tensor1, tensor2, tensor3]) #Stack all 3 tensors along an extra dimension\n",
        "print(stacked_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C6RjqGPqPVu",
        "outputId": "34b08c14-51a5-4006-ad20-dd6d7add1584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6],\n",
            "        [7, 8, 9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrix multiplication"
      ],
      "metadata": {
        "id": "KdwQbaD3uGq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1,2], [3,4], [5,6]])\n",
        "b = torch.tensor([[7,8,9], [10,11,12]])\n",
        "\n",
        "c = a @ b\n",
        "print(c)\n",
        "d = torch.matmul(a, b)\n",
        "print(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoFIptEHuGTo",
        "outputId": "7bfc8da8-d6e6-4217-8c97-5a15d45fa820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 27,  30,  33],\n",
            "        [ 61,  68,  75],\n",
            "        [ 95, 106, 117]])\n",
            "tensor([[ 27,  30,  33],\n",
            "        [ 61,  68,  75],\n",
            "        [ 95, 106, 117]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for the neural network methods and functions. Bias = False introduces no bias term. The first 2 arguments are the dimensions of the weight matrix. This is how you create regular custom layers in pytorch. There are many possible layers (see torch.nn website)"
      ],
      "metadata": {
        "id": "jwDbjADUqbcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "sample = torch.tensor([10., 10., 10.])\n",
        "linear = nn.Linear(3, 3, bias=False) #Create a 3x3 matrix\n",
        "print(linear(sample)) #Whenever applied to a tensor it return the output if it passes through the layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDl--XQBql91",
        "outputId": "3e0099eb-f3bb-4421-b163-492c3a1f9a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.7518, 2.2448, 2.0193], grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you could use the nn.Sequential API and create models exactly how you did in tensorflow (see the website).\n",
        "\n",
        "Below see the softmax function"
      ],
      "metadata": {
        "id": "jsz9F-r1rQUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "tensor1 = torch.tensor([1.0, 2.0, 3.0])\n",
        "softmax_output = F.softmax(tensor1, dim=0) #softmax along the 0th dimension\n",
        "print(softmax_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7jW11OSrvqZ",
        "outputId": "c8b8f3ab-d244-4d61-fc07-d00f24c185c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0900, 0.2447, 0.6652])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embedding\n",
        "\n",
        "You know what embedding is by now. Watch 3b1b if you somehow do not. The embedding dimension is a hyperparameter, basically how complex do you want to make your semantic space. BTW neural network and multilayer perceptron is the same thing."
      ],
      "metadata": {
        "id": "cjoIOvuFsP7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = vocab_size\n",
        "embedding_dim = 100\n",
        "embedding = nn.Embedding(vocab_size, embedding_dim) #Create our mapping function\n",
        "\n",
        "#example\n",
        "random_indices = torch.LongTensor([1, 5, 3, 2])\n",
        "embedded_output = embedding(random_indices)\n",
        "\n",
        "embedded_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5JK_DbRs_f_",
        "outputId": "2c479e34-df5e-46da-9b10-804fcae77875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.6061e-01,  2.4685e-01, -1.4746e+00,  1.7112e+00,  4.5142e-01,\n",
              "          6.7146e-01,  6.3328e-01,  2.2874e+00,  8.2223e-01,  5.4649e-02,\n",
              "         -6.9024e-01, -6.8275e-02, -5.9100e-02, -1.3982e-01, -1.4625e-01,\n",
              "          3.2315e-01, -8.8086e-01,  4.7022e-01, -4.4766e-01, -7.6995e-01,\n",
              "         -7.0201e-01,  1.9830e+00, -1.0658e+00,  5.5441e-01,  7.9698e-02,\n",
              "          1.4904e+00,  2.9600e-01,  6.9858e-01, -1.2551e-01,  5.3266e-01,\n",
              "          1.6305e+00, -7.7768e-02,  1.1295e+00, -1.3841e+00,  1.0300e+00,\n",
              "         -1.5525e+00,  1.3757e+00, -1.2609e-01,  1.8745e-01,  1.6855e+00,\n",
              "         -1.6363e-01,  9.8537e-02,  8.0232e-01, -2.2649e+00, -1.0715e-01,\n",
              "          8.9931e-01, -3.3558e-02, -3.3444e-01,  7.0777e-01, -3.8958e-01,\n",
              "          6.8493e-01,  3.0040e-01, -9.5306e-03,  4.2025e-01,  1.0545e+00,\n",
              "         -1.9577e-01, -6.3084e-01,  1.4094e+00,  1.0459e+00, -8.5410e-01,\n",
              "          2.5702e-01, -1.2885e+00,  2.3138e-01, -7.8347e-01,  3.5477e-02,\n",
              "         -5.2666e-01,  5.5806e-01, -9.4117e-01,  6.6049e-01,  1.3093e-01,\n",
              "         -6.2377e-01, -7.5897e-01, -5.5262e-01, -2.7628e-03, -7.8753e-03,\n",
              "          2.4311e-01, -3.5758e-01, -4.8047e-01, -1.0932e+00,  1.6663e+00,\n",
              "          8.7064e-01, -1.2673e+00, -1.0698e+00,  9.9950e-01, -8.4093e-02,\n",
              "         -9.4704e-01,  9.0903e-01,  7.8834e-01, -2.0486e-01, -1.5968e+00,\n",
              "         -9.6156e-01, -1.5976e-01,  5.4647e-01,  1.5445e+00,  6.8847e-02,\n",
              "          1.8102e+00,  7.4964e-01, -9.5021e-01,  1.5143e+00, -5.6311e-01],\n",
              "        [ 4.3869e-01, -6.9256e-01,  4.8514e-01, -1.3222e+00,  3.9361e-01,\n",
              "         -1.1161e+00,  1.4345e+00, -1.0152e+00,  3.2792e-01,  1.3754e+00,\n",
              "         -7.0403e-01,  7.0367e-01,  2.2249e+00,  6.3706e-01,  3.3386e-01,\n",
              "          3.5783e-01,  3.0441e-01,  5.1345e-01,  1.2067e-01, -4.8478e-01,\n",
              "         -1.1912e+00, -1.5107e-01, -1.5499e+00, -2.6555e+00,  1.3186e-01,\n",
              "          7.4327e-01,  6.9284e-02, -1.4723e+00,  8.6308e-01, -7.3949e-01,\n",
              "          1.0247e+00,  1.0815e+00, -1.1201e+00, -1.0936e+00, -1.2851e-02,\n",
              "         -2.8070e+00, -7.1273e-01,  9.4661e-01,  2.7614e-01,  8.4633e-01,\n",
              "          1.7237e+00, -4.5691e-01,  2.0977e-02, -9.9284e-01,  1.2839e+00,\n",
              "          1.2626e+00, -7.6372e-01, -8.0418e-01,  9.2935e-01, -1.1307e+00,\n",
              "         -7.9577e-01, -1.0776e+00, -5.2917e-01, -1.3995e-01,  7.8913e-01,\n",
              "         -3.8956e-01,  1.1671e+00, -7.6167e-01, -1.0374e+00, -7.6639e-01,\n",
              "          2.1908e+00, -6.2566e-01, -9.3804e-01,  2.1787e-01, -1.9604e-01,\n",
              "          1.5787e+00, -4.5253e-01,  2.6087e-01, -5.2993e-01, -1.3797e+00,\n",
              "         -4.4998e-01,  1.3150e+00,  5.5321e-01,  1.7921e+00, -1.7006e-01,\n",
              "          6.6787e-01, -8.6129e-01, -9.9168e-01,  8.8616e-01,  5.4895e-01,\n",
              "          1.6687e+00, -3.0219e-01, -1.7727e+00,  4.2119e-01,  5.0182e-01,\n",
              "         -2.6287e-01, -9.5769e-01, -9.1299e-01,  1.4919e+00, -3.3523e-01,\n",
              "         -1.5192e+00,  3.7669e-01,  9.0266e-01, -1.5965e+00,  1.3086e+00,\n",
              "         -2.6412e-01, -5.3094e-01,  1.5636e-01, -1.5758e+00,  1.1510e+00],\n",
              "        [-2.4406e-02,  1.3727e+00, -2.8293e-01,  2.3383e-01,  6.1699e-01,\n",
              "         -1.5126e+00,  1.1969e-01,  3.6952e-02, -4.0547e-01, -6.6287e-02,\n",
              "          7.2559e-01,  6.8724e-01,  2.0908e-01, -5.0895e-01, -1.3152e+00,\n",
              "         -5.6728e-03, -1.7307e+00,  4.7323e-01, -5.9849e-01,  1.5216e+00,\n",
              "          1.0276e+00,  4.3782e-01,  6.4031e-01,  3.9623e-01, -6.4610e-01,\n",
              "         -8.3950e-02,  7.1043e-01, -6.1662e-01,  9.3247e-01, -1.8269e+00,\n",
              "          6.0366e-01,  1.1023e+00, -1.4068e+00,  1.0152e+00, -9.1490e-01,\n",
              "          2.0712e+00, -1.2961e+00, -1.6159e-01,  4.1080e-01, -3.4248e+00,\n",
              "         -1.0315e+00, -1.5389e+00,  4.1710e-01, -1.0030e+00,  2.1614e+00,\n",
              "          1.8135e-01, -4.6222e-01, -2.8072e-02, -1.6793e-01, -3.7021e-01,\n",
              "         -3.6825e-01, -1.4737e+00, -1.1454e+00, -8.1229e-02,  2.6050e-01,\n",
              "          3.9009e-01,  4.6756e-03, -2.9800e-01, -1.2160e+00, -2.5191e+00,\n",
              "         -1.6127e+00, -7.0336e-01, -2.6236e-01,  1.2368e+00,  1.1407e-01,\n",
              "         -1.4884e+00,  6.5669e-01, -8.6826e-01, -8.5391e-01,  2.9404e-01,\n",
              "          1.8225e-01,  3.5339e-01, -2.7352e-01,  9.4279e-01, -3.2062e-03,\n",
              "          4.1727e-01,  8.6571e-02,  5.3679e-01,  2.7506e-04,  3.3327e-01,\n",
              "          9.7061e-01, -5.2878e-01, -1.5376e+00, -7.2505e-01,  2.3110e+00,\n",
              "         -1.3536e-02, -6.9222e-01,  1.4960e+00,  6.0514e-01, -6.8446e-01,\n",
              "          3.5972e-01,  2.3721e-01,  3.1224e-01, -7.8913e-01,  1.1711e-01,\n",
              "         -1.1857e+00, -7.9269e-01, -1.9484e-01, -5.0499e-01, -1.6340e+00],\n",
              "        [ 1.2415e+00, -1.2726e+00, -1.1432e-01, -1.1823e+00,  9.7351e-01,\n",
              "         -1.7908e-01,  1.7924e-01,  2.3026e-01, -1.4285e+00,  6.9421e-01,\n",
              "          2.5167e-01,  3.6518e-01,  9.0508e-01, -2.1080e-01, -3.1601e-01,\n",
              "          1.6678e+00, -2.2800e+00, -1.4920e-01,  8.5460e-01, -2.3222e+00,\n",
              "         -1.4835e-01, -7.5740e-01, -4.4578e-01,  1.7258e+00,  5.5567e-01,\n",
              "         -9.8479e-01, -7.9560e-01, -1.0079e+00,  4.4000e-01,  1.3465e+00,\n",
              "         -1.1485e-01, -2.2891e-01,  2.3972e+00, -1.6467e+00,  1.2708e-01,\n",
              "         -6.4755e-01,  1.1495e+00, -1.4974e+00,  1.8794e+00, -3.7894e-01,\n",
              "         -2.7624e-01, -1.0301e-02,  1.4345e+00,  1.8987e-01,  1.6316e+00,\n",
              "         -9.5678e-01,  1.0330e+00,  2.9540e-01, -6.3259e-01,  5.5147e-01,\n",
              "          1.2888e+00,  1.2276e+00,  7.9865e-01, -1.0584e+00, -8.6307e-02,\n",
              "          1.2466e+00, -2.6050e+00,  6.6266e-01,  9.6520e-01,  6.0998e-01,\n",
              "          6.1964e-01, -5.2168e-02,  3.7835e-01, -4.0622e-01,  1.6019e+00,\n",
              "         -2.5779e-01,  1.3932e+00, -1.4077e-02,  2.0965e-01, -7.3712e-01,\n",
              "         -1.1132e+00,  1.8105e-01,  5.9449e-01, -1.7190e+00, -4.8149e-01,\n",
              "          5.3528e-01,  1.9923e+00, -8.1141e-01, -3.3597e-01, -8.5285e-02,\n",
              "          8.7996e-01, -7.4563e-01, -1.1390e+00,  1.6745e+00, -1.5370e-01,\n",
              "          6.3627e-01,  1.2562e-01, -9.4904e-01,  1.1147e+00, -7.8231e-01,\n",
              "         -1.2387e+00,  1.9453e-01, -3.8440e-01, -1.7127e+00, -1.2542e+00,\n",
              "         -1.8339e+00, -6.1683e-01, -8.1575e-01,  1.3634e+00, -2.8627e-01]],\n",
              "       grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Training and Development (Bigram)"
      ],
      "metadata": {
        "id": "V2ftzy22wWKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use nll as our loss function and the adam W optimizer (look at torch optimizers page). Adam W leads to better regularization.\n",
        "\n",
        "Also we write the forward pass function (forward pass through the neural network) from scratch ourselves because you can understand how the data gets transformed step by step, and you have freedom to create your own layers.\n",
        "\n",
        "We need to reshape the logits. C is the channel size (vocabulary size). The .view method in pytorch reshapes a tensor without changing its data. The purpose of the reshaping is so that the shapes are compatible for the cross entropy function (see what cross entropy expects).\n",
        "\n",
        "It expects a C in the 2nd shape argument."
      ],
      "metadata": {
        "id": "dWHrUifLw1-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(2, 3, 5)\n",
        "a.view(30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3x7wdEEphWS",
        "outputId": "d8bae4fe-1b03-4e8d-f303-517e79dd65b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5327, 0.8851, 0.2264, 0.3411, 0.0833, 0.5025, 0.8800, 0.0619, 0.2684,\n",
              "        0.1377, 0.2342, 0.7635, 0.0441, 0.2077, 0.7670, 0.4485, 0.2564, 0.8747,\n",
              "        0.0882, 0.2180, 0.1531, 0.8768, 0.3880, 0.8192, 0.3223, 0.5152, 0.7320,\n",
              "        0.9656, 0.6906, 0.8787])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad() #a decorator telling python no need gradients at all here, saves time and memory\n",
        "#useful for inference\n",
        "\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval() #set the model to evaluation mode\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model.forward(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train() #set back to training mode\n",
        "  return out"
      ],
      "metadata": {
        "id": "RWEqXqbd0SAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module): #inherit as a subclass so nn.Linear is learnable\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) #Create the embedding layer\n",
        "\n",
        "  def forward(self, index, targets=None):\n",
        "    logits = self.token_embedding_table(index) #map the word index to a vector called logits, shape is BxTxC\n",
        "    #each entry is a sequences, each sequence contains word ids, each word id is mapped to the embedding\n",
        "    #vector supposed to be saying the probability distribution over all words for the next word\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape #batch size, time, channel\n",
        "      logits = logits.view(B*T, C) #blend the batch and time dimensions\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets) #computes cross entropy between the probabilities and the actual targets\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, index, max_new_tokens): #generate new tokens (max_new is the number of new tokens)\n",
        "    #index is (B, T) a bunch of sequences\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self.forward(index) #Call a forward pass, output is now BxTxC for logits\n",
        "      logits = logits[:, -1, :] #Take the last timestep (word in each sequence) as its a bigram model, becomes a B x C shape\n",
        "      probs = F.softmax(logits, dim=-1) #softmax over the last dimension ('embeddedment')\n",
        "      index_next = torch.multinomial(probs, num_samples=1) #sample from the distribution\n",
        "      index = torch.cat((index, index_next), dim=1) #append to the sequence, autoregressive format\n",
        "    return index\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device) #move model to GPU\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "id": "UQUTITn0waCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0e93d79-aac9-4572-838b-4e6d974ed469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "abv]iP4YsN'BCQ'0yrNnYCsmH(iPb;\n",
            "X-RDKMYP2Pb1I7-LOGgbC),sGA660M?zOfrd-?cexea﻿TBfhbQD(Vg*erT6 RLZ!4NOmSp6hQz8qrrxErT45NO﻿:ik5t[zL\n",
            "MP,&\"wA?_TtWRwb9!y51!CK.]RMZK(F\n",
            "S,-WTtS)0FLUQWSCZFls8675N5s745dGB]2wbK[ :*5-4t*rmKXPsfaSZqZ?]vnxmE**5)WoxNqDRvwgJB-0Gfp.AswHda,IhYj*5:'q2DI89Ys2T&a'MS,AgR5:J_-4tj-hMRv[zLGBY:;BFK)3RY6EzXJ3[9Wy,sis?mn7mnA\"﻿qm5HDB](VE&\"6dnZ)?a,-,S)iyr8\n",
            "﻿S,YLR[]n]cqT!y10'_..,AE*?!Hm-T55(3RY*TZDI5z(CMTtT5D540sD;?p0[*ycN;)Dop\n",
            "sw;T*nP&l&O\n",
            "GxRIaPK(Vwt5U4yxMmgRWVJqBea0wOcOEpj3Fc)isy)Q&Qv&2TQgBw:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So whats going on? Even though typically the embedding layer acts as a means to convert a word into a vector in such a way so as to encode semantic meanings in directions. Here the embedding layer acts as a cocurrent matrix because of the way we designed the model architecture. Neural nets will just adapt the weights necessary so as to achieve the task (optimize the cost function) so this embedding layer will end up adjusting itself to be a probability distribution of a word given another word because of our architecture. So embedding is not exactly the right word but technically it is because the word is converted to a continuous vector, with each entry correlating with the probability distribution of other words given the current word. Hence why its vocab size by vocab size. For a trigram it would be a 3d tensor or something.\n",
        "\n",
        "So its called logits because its before the softmax. So while training the batches are gotten, passed into the forward pass, and the loss computed between each element's probability distribution and the target word index. This is why we didn't embed the target word (its not a direct embedding).\n",
        "\n",
        "Then we tack on the new index as it's a bigram"
      ],
      "metadata": {
        "id": "qDRcRJLnrcOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now define the optimizer, AdamW and the first argument pass model.paramaters and the 2nd pass the learning rate (see website).\n",
        "\n",
        "Typical optimizers:\n",
        "\n",
        "1. Mean Squared Error (mainly for regression)\n",
        "2. Gradient Descent (just standard grad desc)\n",
        "3. Momentum is an extension of stochastic gradient descent (helps smooth out changes and allows it to continuing movigng in right direction even if gradient changes direction), (good is like 90% of previous gradient and 10% of current one)\n",
        "4. RMSprop (uses a moving average of the squared gradients of each parameter, avoids oscillations and can improve convergence)\n",
        "5. Adam combines momentum and rms prop\n",
        "6. AdamW adds weight decay to regularize and improve generalization."
      ],
      "metadata": {
        "id": "xCfLv7LbrhTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 3e-4\n",
        "max_iters = 10000\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "-0dnD69Qri-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call the main loop. We zero the grad because pytorch automatically accumulates the gradients over time and we do not want that. We only want to optimize based on our current data. Instead of setting it to zero we set it to None as it occupies much less space. Typically for RNNs we don't zero grad, we do gradient accumulation.\n",
        "\n",
        "\n",
        "Also do not compute the loss every step as its noisy for each batch (since random fluctuations) so do every number of eval_iters."
      ],
      "metadata": {
        "id": "YeUfoiNDr8FU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_iters = 1000\n",
        "max_iters = 2000\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_iters == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train_loss: {losses['train']:.4f}, val_loss: {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model.forward(xb, yb) #Do a forward pass\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward() #Back propagate to compute the gradients\n",
        "  optimizer.step() #Step the gradient\n",
        "\n",
        "print(loss.item()) #Loss.item converts tensor to python float"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOoPcb3Jr9Wt",
        "outputId": "83c5c11b-ff9a-41f4-8f2d-74bf028a6904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train_loss: 4.8881, val_loss: 4.8862\n",
            "step 1000: train_loss: 4.6436, val_loss: 4.6398\n",
            "4.547786712646484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate text, we get gibbrish since its a bigram model which doesn't take much context."
      ],
      "metadata": {
        "id": "Va9L2dbEtFjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "id": "VYmXbRm2tM19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "932da20f-1219-4151-9348-d93eacbbe2a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "p'E55qNlDY,?9P&j[1tTL﻿qK﻿n.9IjV[ !PtGi.SyKjNM)So;?vrarebnsvA6*ceb:FM.\"uCkiPYKV873p'QHoflI\"n95FH\"﻿xAQ\" jDgt[&2dGB4esMmor*[:1s51Od1q:eZ0apGXXfqo_?4SL?Dw1ntuSgxrd1?MlrR3O0!s?nT﻿DHdX*Yamgte;8*WS8-)W45mFfytiJq0SFk5Q1?﻿-nc4\"X73xFqN'QDY1083-I45,WRYAISgbq6d_iZzbz7vN&5!P,*LYvrkn\n",
            "h8aQg5MwQhouF Y ! Dg.ngISZKM6q(RmzbEBTZZwq90Bl7W9yuCM6vgoR9w7NPt0BH8UWRY\n",
            "PraNo&L\"7A](EzL?4me6j[T68?9ILu_w3A1f7_meE_t](CHgQC tfCUMviOatpetor0 hAt*3'K(yM84mQ\"kshMZ5y2ZKElPYPqD_lId1btf&ltf.T?mmoi2hOX)(&??gM\"KMmeHL(KMlmf.bj)iYK8UuqNM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note on activation functions**\n",
        "\n",
        "tanh is used over sigmoid to counter the vanishing gradient problem, and the mean is 0 so is used for multilayer perceptrons."
      ],
      "metadata": {
        "id": "eXeadv5E5HeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Architecture and Training (Transformers)\n",
        "\n",
        "**Residual Neural Networks**\n",
        "\n",
        "Allows jumps, i.e the input layer is added to the current layer. This is done in order to make sure the network doesn't forget the earlier words/layers in the network. There are 2 ways this could be implemented, it could be normalized first and then added or add the layer and then normalize together (the 2nd way works better)\n",
        "\n",
        "**Multi-head attention**\n",
        "\n",
        "For attention mechanism see 3b1b or see lecture notes from cambridge, essentially keys, queries, values, softmax, take into account how context affects word embeddings. Query is like a question, query dimension is a hyperparameter equal to head_size. And then normalize by dividing by square root of the dimensions. This is done to keep the variance approximately constant so the softmax function does not blow up (numerical stability). We do not want softmax to sharpen too near initialization as then we're only taking data from one node/token.\n",
        "\n",
        "We use multi-heads because you can imagine it as many different people reading a book and learning different perspectives and takes (different queries and keys asking different questions and answers about the input). Then we combine their results.\n",
        "\n",
        "Masked attention doesn't use the future words for the training data. Also allows you to essentially get multiple training samples from one sequence, and the model can see many context sizes. In the transformer architecture in the google \"Attention is all you need\" paper, masked attention is only used once out of the 3 multihead attention sections.\n",
        "\n",
        "**Encoder**\n",
        "\n",
        "The purpose of an encoder is to learn the present, past and future and put that into a vector representation for the decoder, so its ok if we use the future data here. We used masked attention for the decoder\n",
        "\n",
        "\n",
        "**nn.Linear**\n",
        "\n",
        "The nn.Linear layers effectively act as some form of data compression and dimensionality reduction where lots and lots of data are reduced to less data, effectively summarizing the previous process. ReLU is typically used for this as you're just transferring the data. Keep this mind when doing future neural network architecture."
      ],
      "metadata": {
        "id": "BpqzjKy9M7Tg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embedding and Positional Encoding**\n",
        "\n",
        "Words are first converted into a dense vector by multiplying some sort of embedding matrix. Then the positional information is also encoded in (as transformer doesn't do that automatically like RNN) so we use these encodings to tell the order of the sequence. We use a trigonometric formula for each dimension to ensure each position has a unique encode.\n",
        "\n",
        "**The Encoder Stage**\n",
        "\n",
        "It processes the input sequence. It consists of a:\n",
        "\n",
        "1. multi-head self attention layer\n",
        "2. Residual connection and norm\n",
        "3. position-wise feed forward network\n",
        "4. Add and norm\n",
        "\n",
        "After the multi-head self attention, which is meant to encode the embedding of the words given the context of the words around it, the residual connection (from the original input) is added, then layer normalization occurs. This preserves gradient flow. Each embedding word in the output passes through a separate feed forward networks, consisting of 2 linear transformations with a relu activation in between. The first step typically increases the dimensionality of the data (less to more neurons) which increases the capacity of the network to capture complex features and patterns, then we apply ReLU to introduce non-linearity essentially cutting the less important features and then bringing the model back down to the original dimension to integrate with the rest of the model but now it has some of the data of the expanded features with encodings of some more complex data, this is like feature extraction.\n",
        "\n",
        "After that the a residual connection from the input to the feed forward layer is added and layer normalization is applied to stabilize and speed up training.\n",
        "\n",
        "\n",
        "**Decoding Stage**\n",
        "\n",
        "The input to the decoder is the target sequence (shifted by one) so the decoder predicts the next word in the sequence given the previous words.\n",
        "\n",
        "1. Masked multi-head self attention\n",
        "2. Residual connection and norm\n",
        "3. multi-head cross attention\n",
        "4. Add and norm\n",
        "5. Feed forward network\n",
        "6. Add and Norm\n",
        "7. Linear Transformation and Softmax\n",
        "\n",
        "Now we need to include the masked version as each position can only consider information from earlier positions and there should be no information transfer from future tokens. This also turns one training sequence into multiple.\n",
        "\n",
        "Then we do cross attention where the keys and values come from the encoded inputs and the queries come from the decoder. This allows the decoder to understand the information from the entire input sequence processed by the encoder. Then the rest is self-explanatory. The linear layer transforms the decoder's outputs into logits, its good to separate things like this so as to not provide too many functions to a single layer.\n",
        "\n",
        "\n",
        "Now we're building a GPT not just a transformer. The GPT only uses the decoder part of the transformer architecture which is good for sequence generation. The original transformer includes both which is good for understanding and generating sequences (machine translation). GPT is pre-trained to predict the next word in a sequence and later fine tuned for specific tasks. Also in text generation, GPT is autoregressive whereas transformer encodes the entire input and decoder processes this entire information to generate output"
      ],
      "metadata": {
        "id": "PfReFylqQkDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now before we get into that there's also the average method where we average the words where for every batch, every t-th token, we average all the vectors in all the previous tokens, including the current one. But doesn't account for importance at all."
      ],
      "metadata": {
        "id": "-Ikgl7O1Eyz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #averaging- bag of words\n",
        "# xbow = torch.zeros((B,T,C))\n",
        "# for b in range(B):\n",
        "#   for t in range(T):\n",
        "#     xprev = x[b,:t+1] #all previous elements\n",
        "#     xbow[b,t] = torch.mean(xprev, 0) #mean over all previous elements\n",
        "\n",
        "# x[0]"
      ],
      "metadata": {
        "id": "08jaudZNFJTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #its more efficient to do it as matrix multiplication\n",
        "#  #tril returns lower triangular part of a matrix\n",
        "# #represent this sequence averaging as\n",
        "# a = torch.tril(torch.ones(3,3))\n",
        "# a = a/torch.sum(a, 1, keepdim=True) #sum along axis 1\n",
        "# b = torch.randint(0,10,(3,2)).float() #converts to float\n",
        "# c = a @ b\n",
        "# print(a)\n",
        "# print(b)\n",
        "# print(c)"
      ],
      "metadata": {
        "id": "9Afw3EO1FM16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wei = torch.tril(torch.ones(T, T))\n",
        "# wei = wei / wei.sum(1, keepdim=True)\n",
        "# xbow2 = wei @ x # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "# torch.allclose(xbow, xbow2) #Check if these 2 are the same"
      ],
      "metadata": {
        "id": "vocHhBFxFSHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Again but instead of standard normalization, apply softmax\n",
        "# tril = torch.tril(torch.ones(T, T))\n",
        "# wei = torch.zeros((T, T))\n",
        "# wei - wei.masked_fill(tril == 0, float('-inf')) #wherever tril is 0, set to -inf (see 3b1b), tokens from past cannot communicate\n",
        "# wei = F.softmax(wei, dim=-1)\n",
        "# xbow3 = wei @ x\n",
        "# torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "id": "KjYKPuZ8FT2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now **ATTENTION**"
      ],
      "metadata": {
        "id": "jFe9MOO-Fb3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "7wkAeIAtOWt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_iters = 3000\n",
        "eval_iters = 1\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "n_layer = 8\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "batch_size = 128\n",
        "block_size = 64\n",
        "dropout = 0.25 #20% of neurons will dropout\n",
        "\n",
        "@torch.no_grad()\n",
        "\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model.forward(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "kPQO8pX3ssJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're using multiple decoder blocks in our GPT sequentially."
      ],
      "metadata": {
        "id": "C8UZ4I1otzO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd) #We're going to make the positional encodings learnable rather than sinusoidal\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)]) #4 decoder blocks\n",
        "\n",
        "    self.ln_f = nn.LayerNorm(n_embd) #Final layer norm at the end of all decoding blocks\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size) #This transforms the embeddings into the vocabularies so we can do softmax\n",
        "\n",
        "    self.apply(self.__init__weights) #initialize our weight, self.apply is provided by pytorch (inherited)\n",
        "\n",
        "  def __init__weights(self, module):\n",
        "    if isinstance(module, nn.Linear): #if its a linear thing\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias) #assign 0 to the biases\n",
        "    elif isinstance(module, nn.Embedding): #if its embedding\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, index, targets=None):\n",
        "    tok_emb = self.token_embedding_table(index)\n",
        "    B, T, C = tok_emb.shape\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device)) #move to GPU\n",
        "    x = tok_emb + pos_emb #add the positional embeddings to the token embeddings to encode both behavior\n",
        "\n",
        "    #Now we have the embeddings so we feed it into our blocks\n",
        "\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, index, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self.forward(index[:,-block_size:]) if _ >= block_size else self.forward(index)\n",
        "      logits = logits[:, -1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      index_next = torch.multinomial(probs, num_samples=1)\n",
        "      index = torch.cat((index, index_next), dim=1)\n",
        "    return index"
      ],
      "metadata": {
        "id": "3JM36AkcswD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to generate the block class (the decoders)"
      ],
      "metadata": {
        "id": "bTeZALh9w6Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head #headsize is number of features each head captures, its essentially the query dimension\n",
        "    self.sa = MultiHeadAttention(n_head, head_size) #This is another class we need to create\n",
        "    self.ffwd = FeedForward(n_embd) #Another class\n",
        "    self.ln1 = nn.LayerNorm(n_embd) #For the attention\n",
        "    self.ln2 = nn.LayerNorm(n_embd) #For the feed forward\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    y = self.sa(x)\n",
        "    x = self.ln1(x + y) #Add and norm, our residual connection\n",
        "    y = self.ffwd(x)\n",
        "    x = self.ln2(x + y) #Add and norm, so we need 2 variables x and y for resnet\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "VNWG2_Opw5gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the Feed Forward and Multihead attention"
      ],
      "metadata": {
        "id": "LgDCPsFZx-G7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential( #define the net with sequential api\n",
        "        nn.Linear(n_embd, 4 * n_embd), #dimensionality increase\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "        nn.Dropout(dropout) #Add dropout to prevent overfitting\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) #Create a list of heads\n",
        "    self.proj = nn.Linear(head_size * num_heads, n_embd) #project so its ready for next layer, we only put this so if we change one of these there won't be dimensional errors\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1) #concatenate the heads\n",
        "    out = self.dropout(self.proj(out)) #Applies the layers to each element in tensor\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False) #no biases in the attention\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #registers a tensor as a buffer in the model (not parameters so not updated so easier to load)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5 #a form of normalization\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "a2SWcaJxx_2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to train our model. Just a note: module list and sequential are different. Module List is just a list of modules, not in any particular order. It can levarage arraywise operations"
      ],
      "metadata": {
        "id": "6V9P1Dxr1p3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "nWoiqXnaitXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "max_iters = 500\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  if iter % eval_iters == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"step {iter}: train_loss: {losses['train']:.4f}, val_loss: {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  logits, loss = model.forward(xb, yb) #Do a forward pass\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward() #Back propagate to compute the gradients\n",
        "  optimizer.step() #Step the gradient\n",
        "\n",
        "print(loss.item()) #Loss.item converts tensor to python float\n",
        "\n",
        "with open('model-01.pkl', 'wb') as f: #wb = write binary\n",
        "  pickel.dump(model, f)\n",
        "  print('model saved') #saves everything about the model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0Vm_VPC1sXy",
        "outputId": "14e19bc8-e98e-48f5-9ed1-9e4769aa3d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14.274129 M parameters\n",
            "step 0: train_loss: 4.4398, val_loss: 4.4351\n",
            "step 1: train_loss: 3.5471, val_loss: 3.5422\n",
            "step 2: train_loss: 3.2923, val_loss: 3.3172\n",
            "step 3: train_loss: 3.2025, val_loss: 3.2248\n",
            "step 4: train_loss: 3.1244, val_loss: 3.1297\n",
            "step 5: train_loss: 3.0778, val_loss: 3.0524\n",
            "step 6: train_loss: 2.9922, val_loss: 2.9843\n",
            "step 7: train_loss: 2.9284, val_loss: 2.9830\n",
            "step 8: train_loss: 2.8634, val_loss: 2.8930\n",
            "step 9: train_loss: 2.8611, val_loss: 2.8991\n",
            "step 10: train_loss: 2.8186, val_loss: 2.8243\n",
            "step 11: train_loss: 2.7899, val_loss: 2.7805\n",
            "step 12: train_loss: 2.6887, val_loss: 2.7739\n",
            "step 13: train_loss: 2.7487, val_loss: 2.7103\n",
            "step 14: train_loss: 2.7091, val_loss: 2.7108\n",
            "step 15: train_loss: 2.6697, val_loss: 2.7381\n",
            "step 16: train_loss: 2.6862, val_loss: 2.6956\n",
            "step 17: train_loss: 2.6652, val_loss: 2.6957\n",
            "step 18: train_loss: 2.6345, val_loss: 2.7164\n",
            "step 19: train_loss: 2.6781, val_loss: 2.6547\n",
            "step 20: train_loss: 2.5695, val_loss: 2.6496\n",
            "step 21: train_loss: 2.5581, val_loss: 2.6601\n",
            "step 22: train_loss: 2.6135, val_loss: 2.6592\n",
            "step 23: train_loss: 2.5696, val_loss: 2.6060\n",
            "step 24: train_loss: 2.5827, val_loss: 2.6435\n",
            "step 25: train_loss: 2.5897, val_loss: 2.6289\n",
            "step 26: train_loss: 2.5559, val_loss: 2.6246\n",
            "step 27: train_loss: 2.5013, val_loss: 2.5781\n",
            "step 28: train_loss: 2.5411, val_loss: 2.5875\n",
            "step 29: train_loss: 2.5661, val_loss: 2.6066\n",
            "step 30: train_loss: 2.4968, val_loss: 2.6046\n",
            "step 31: train_loss: 2.5513, val_loss: 2.5524\n",
            "step 32: train_loss: 2.5118, val_loss: 2.5658\n",
            "step 33: train_loss: 2.4958, val_loss: 2.5634\n",
            "step 34: train_loss: 2.4866, val_loss: 2.5657\n",
            "step 35: train_loss: 2.5127, val_loss: 2.5736\n",
            "step 36: train_loss: 2.4737, val_loss: 2.5510\n",
            "step 37: train_loss: 2.4847, val_loss: 2.5585\n",
            "step 38: train_loss: 2.4801, val_loss: 2.5527\n",
            "step 39: train_loss: 2.4889, val_loss: 2.5501\n",
            "step 40: train_loss: 2.4951, val_loss: 2.5699\n",
            "step 41: train_loss: 2.4910, val_loss: 2.5683\n",
            "step 42: train_loss: 2.4879, val_loss: 2.5359\n",
            "step 43: train_loss: 2.4671, val_loss: 2.5186\n",
            "step 44: train_loss: 2.4501, val_loss: 2.5093\n",
            "step 45: train_loss: 2.4933, val_loss: 2.5635\n",
            "step 46: train_loss: 2.4768, val_loss: 2.5062\n",
            "step 47: train_loss: 2.4933, val_loss: 2.5250\n",
            "step 48: train_loss: 2.4213, val_loss: 2.5296\n",
            "step 49: train_loss: 2.4542, val_loss: 2.5177\n",
            "step 50: train_loss: 2.4766, val_loss: 2.5245\n",
            "step 51: train_loss: 2.4653, val_loss: 2.5153\n",
            "step 52: train_loss: 2.4606, val_loss: 2.4885\n",
            "step 53: train_loss: 2.4309, val_loss: 2.4810\n",
            "step 54: train_loss: 2.4333, val_loss: 2.5132\n",
            "step 55: train_loss: 2.4457, val_loss: 2.4957\n",
            "step 56: train_loss: 2.4091, val_loss: 2.4764\n",
            "step 57: train_loss: 2.4754, val_loss: 2.5125\n",
            "step 58: train_loss: 2.4305, val_loss: 2.5244\n",
            "step 59: train_loss: 2.3822, val_loss: 2.4999\n",
            "step 60: train_loss: 2.4191, val_loss: 2.5009\n",
            "step 61: train_loss: 2.4357, val_loss: 2.5033\n",
            "step 62: train_loss: 2.3783, val_loss: 2.4921\n",
            "step 63: train_loss: 2.4309, val_loss: 2.4656\n",
            "step 64: train_loss: 2.4136, val_loss: 2.5011\n",
            "step 65: train_loss: 2.3989, val_loss: 2.4974\n",
            "step 66: train_loss: 2.4160, val_loss: 2.4932\n",
            "step 67: train_loss: 2.4205, val_loss: 2.4877\n",
            "step 68: train_loss: 2.4080, val_loss: 2.4921\n",
            "step 69: train_loss: 2.4198, val_loss: 2.4424\n",
            "step 70: train_loss: 2.4238, val_loss: 2.4433\n",
            "step 71: train_loss: 2.4158, val_loss: 2.4645\n",
            "step 72: train_loss: 2.4161, val_loss: 2.4534\n",
            "step 73: train_loss: 2.3792, val_loss: 2.4691\n",
            "step 74: train_loss: 2.3826, val_loss: 2.4642\n",
            "step 75: train_loss: 2.3861, val_loss: 2.4496\n",
            "step 76: train_loss: 2.3986, val_loss: 2.4881\n",
            "step 77: train_loss: 2.3667, val_loss: 2.4432\n",
            "step 78: train_loss: 2.4008, val_loss: 2.4570\n",
            "step 79: train_loss: 2.3593, val_loss: 2.4640\n",
            "step 80: train_loss: 2.3902, val_loss: 2.4139\n",
            "step 81: train_loss: 2.3796, val_loss: 2.4378\n",
            "step 82: train_loss: 2.3701, val_loss: 2.4599\n",
            "step 83: train_loss: 2.3564, val_loss: 2.4340\n",
            "step 84: train_loss: 2.3634, val_loss: 2.4296\n",
            "step 85: train_loss: 2.3363, val_loss: 2.4421\n",
            "step 86: train_loss: 2.3282, val_loss: 2.4529\n",
            "step 87: train_loss: 2.3935, val_loss: 2.4440\n",
            "step 88: train_loss: 2.3519, val_loss: 2.4090\n",
            "step 89: train_loss: 2.3575, val_loss: 2.4358\n",
            "step 90: train_loss: 2.3718, val_loss: 2.4172\n",
            "step 91: train_loss: 2.3607, val_loss: 2.4267\n",
            "step 92: train_loss: 2.3729, val_loss: 2.4351\n",
            "step 93: train_loss: 2.3565, val_loss: 2.4215\n",
            "step 94: train_loss: 2.3468, val_loss: 2.3771\n",
            "step 95: train_loss: 2.3471, val_loss: 2.3718\n",
            "step 96: train_loss: 2.3767, val_loss: 2.3819\n",
            "step 97: train_loss: 2.3642, val_loss: 2.4026\n",
            "step 98: train_loss: 2.3183, val_loss: 2.4214\n",
            "step 99: train_loss: 2.3392, val_loss: 2.3866\n",
            "step 100: train_loss: 2.3047, val_loss: 2.3841\n",
            "step 101: train_loss: 2.3068, val_loss: 2.4239\n",
            "step 102: train_loss: 2.3288, val_loss: 2.3833\n",
            "step 103: train_loss: 2.3634, val_loss: 2.3780\n",
            "step 104: train_loss: 2.3304, val_loss: 2.3834\n",
            "step 105: train_loss: 2.3478, val_loss: 2.3899\n",
            "step 106: train_loss: 2.3151, val_loss: 2.4216\n",
            "step 107: train_loss: 2.3246, val_loss: 2.3814\n",
            "step 108: train_loss: 2.3180, val_loss: 2.3747\n",
            "step 109: train_loss: 2.2962, val_loss: 2.3698\n",
            "step 110: train_loss: 2.3345, val_loss: 2.3592\n",
            "step 111: train_loss: 2.3213, val_loss: 2.3836\n",
            "step 112: train_loss: 2.3020, val_loss: 2.3770\n",
            "step 113: train_loss: 2.3070, val_loss: 2.3710\n",
            "step 114: train_loss: 2.3145, val_loss: 2.3458\n",
            "step 115: train_loss: 2.2995, val_loss: 2.3779\n",
            "step 116: train_loss: 2.2626, val_loss: 2.3731\n",
            "step 117: train_loss: 2.2910, val_loss: 2.3720\n",
            "step 118: train_loss: 2.2782, val_loss: 2.3576\n",
            "step 119: train_loss: 2.2662, val_loss: 2.3516\n",
            "step 120: train_loss: 2.2583, val_loss: 2.3409\n",
            "step 121: train_loss: 2.2404, val_loss: 2.3319\n",
            "step 122: train_loss: 2.3013, val_loss: 2.3531\n",
            "step 123: train_loss: 2.2652, val_loss: 2.3271\n",
            "step 124: train_loss: 2.2533, val_loss: 2.3382\n",
            "step 125: train_loss: 2.2485, val_loss: 2.3061\n",
            "step 126: train_loss: 2.2431, val_loss: 2.3426\n",
            "step 127: train_loss: 2.2575, val_loss: 2.3173\n",
            "step 128: train_loss: 2.2325, val_loss: 2.3289\n",
            "step 129: train_loss: 2.2278, val_loss: 2.2962\n",
            "step 130: train_loss: 2.2463, val_loss: 2.3119\n",
            "step 131: train_loss: 2.2193, val_loss: 2.2698\n",
            "step 132: train_loss: 2.1765, val_loss: 2.3021\n",
            "step 133: train_loss: 2.2279, val_loss: 2.2788\n",
            "step 134: train_loss: 2.1654, val_loss: 2.2756\n",
            "step 135: train_loss: 2.1718, val_loss: 2.2908\n",
            "step 136: train_loss: 2.1895, val_loss: 2.2870\n",
            "step 137: train_loss: 2.2225, val_loss: 2.2788\n",
            "step 138: train_loss: 2.1929, val_loss: 2.2708\n",
            "step 139: train_loss: 2.1868, val_loss: 2.2816\n",
            "step 140: train_loss: 2.1703, val_loss: 2.2704\n",
            "step 141: train_loss: 2.1675, val_loss: 2.2439\n",
            "step 142: train_loss: 2.1848, val_loss: 2.2758\n",
            "step 143: train_loss: 2.1781, val_loss: 2.2115\n",
            "step 144: train_loss: 2.1450, val_loss: 2.2077\n",
            "step 145: train_loss: 2.1618, val_loss: 2.2502\n",
            "step 146: train_loss: 2.1494, val_loss: 2.1926\n",
            "step 147: train_loss: 2.1603, val_loss: 2.2315\n",
            "step 148: train_loss: 2.1676, val_loss: 2.2224\n",
            "step 149: train_loss: 2.1415, val_loss: 2.2678\n",
            "step 150: train_loss: 2.1277, val_loss: 2.2329\n",
            "step 151: train_loss: 2.1116, val_loss: 2.2109\n",
            "step 152: train_loss: 2.1205, val_loss: 2.2183\n",
            "step 153: train_loss: 2.1583, val_loss: 2.2175\n",
            "step 154: train_loss: 2.1083, val_loss: 2.1762\n",
            "step 155: train_loss: 2.1116, val_loss: 2.2146\n",
            "step 156: train_loss: 2.1265, val_loss: 2.1978\n",
            "step 157: train_loss: 2.1134, val_loss: 2.2251\n",
            "step 158: train_loss: 2.1075, val_loss: 2.2066\n",
            "step 159: train_loss: 2.1130, val_loss: 2.1571\n",
            "step 160: train_loss: 2.0965, val_loss: 2.1983\n",
            "step 161: train_loss: 2.0614, val_loss: 2.1892\n",
            "step 162: train_loss: 2.0966, val_loss: 2.1712\n",
            "step 163: train_loss: 2.0815, val_loss: 2.1947\n",
            "step 164: train_loss: 2.0810, val_loss: 2.1736\n",
            "step 165: train_loss: 2.0867, val_loss: 2.1950\n",
            "step 166: train_loss: 2.0653, val_loss: 2.1588\n",
            "step 167: train_loss: 2.0589, val_loss: 2.2013\n",
            "step 168: train_loss: 2.0119, val_loss: 2.1635\n",
            "step 169: train_loss: 2.0627, val_loss: 2.1414\n",
            "step 170: train_loss: 2.0960, val_loss: 2.1606\n",
            "step 171: train_loss: 2.0652, val_loss: 2.1386\n",
            "step 172: train_loss: 2.0680, val_loss: 2.1694\n",
            "step 173: train_loss: 2.0654, val_loss: 2.1249\n",
            "step 174: train_loss: 2.0328, val_loss: 2.1380\n",
            "step 175: train_loss: 2.0395, val_loss: 2.1321\n",
            "step 176: train_loss: 2.0325, val_loss: 2.1297\n",
            "step 177: train_loss: 2.0264, val_loss: 2.0977\n",
            "step 178: train_loss: 2.0159, val_loss: 2.1182\n",
            "step 179: train_loss: 2.0310, val_loss: 2.1106\n",
            "step 180: train_loss: 2.0457, val_loss: 2.1221\n",
            "step 181: train_loss: 1.9509, val_loss: 2.1159\n",
            "step 182: train_loss: 2.0387, val_loss: 2.1072\n",
            "step 183: train_loss: 2.0205, val_loss: 2.1381\n",
            "step 184: train_loss: 1.9932, val_loss: 2.1087\n",
            "step 185: train_loss: 1.9703, val_loss: 2.0997\n",
            "step 186: train_loss: 2.0261, val_loss: 2.0985\n",
            "step 187: train_loss: 1.9694, val_loss: 2.0996\n",
            "step 188: train_loss: 1.9960, val_loss: 2.0889\n",
            "step 189: train_loss: 1.9990, val_loss: 2.0593\n",
            "step 190: train_loss: 1.9529, val_loss: 2.1181\n",
            "step 191: train_loss: 1.9852, val_loss: 2.0666\n",
            "step 192: train_loss: 1.9669, val_loss: 2.0977\n",
            "step 193: train_loss: 1.9625, val_loss: 2.0948\n",
            "step 194: train_loss: 1.9637, val_loss: 2.0780\n",
            "step 195: train_loss: 1.9394, val_loss: 2.0876\n",
            "step 196: train_loss: 1.9519, val_loss: 2.0601\n",
            "step 197: train_loss: 1.9207, val_loss: 2.0602\n",
            "step 198: train_loss: 1.9566, val_loss: 2.0415\n",
            "step 199: train_loss: 1.9605, val_loss: 2.0677\n",
            "step 200: train_loss: 1.9546, val_loss: 2.0643\n",
            "step 201: train_loss: 1.9196, val_loss: 2.0530\n",
            "step 202: train_loss: 1.9021, val_loss: 2.0758\n",
            "step 203: train_loss: 1.9162, val_loss: 2.0457\n",
            "step 204: train_loss: 1.9603, val_loss: 2.0835\n",
            "step 205: train_loss: 1.9557, val_loss: 2.0411\n",
            "step 206: train_loss: 1.9164, val_loss: 2.0639\n",
            "step 207: train_loss: 1.8924, val_loss: 2.0413\n",
            "step 208: train_loss: 1.9346, val_loss: 2.0260\n",
            "step 209: train_loss: 1.9167, val_loss: 2.0062\n",
            "step 210: train_loss: 1.9210, val_loss: 1.9969\n",
            "step 211: train_loss: 1.9582, val_loss: 2.0348\n",
            "step 212: train_loss: 1.8934, val_loss: 2.0496\n",
            "step 213: train_loss: 1.8828, val_loss: 2.0048\n",
            "step 214: train_loss: 1.9128, val_loss: 2.0606\n",
            "step 215: train_loss: 1.8872, val_loss: 2.0203\n",
            "step 216: train_loss: 1.8866, val_loss: 1.9808\n",
            "step 217: train_loss: 1.8733, val_loss: 1.9904\n",
            "step 218: train_loss: 1.8760, val_loss: 2.0082\n",
            "step 219: train_loss: 1.8755, val_loss: 2.0277\n",
            "step 220: train_loss: 1.8805, val_loss: 2.0422\n",
            "step 221: train_loss: 1.8673, val_loss: 2.0120\n",
            "step 222: train_loss: 1.8649, val_loss: 1.9724\n",
            "step 223: train_loss: 1.8912, val_loss: 2.0464\n",
            "step 224: train_loss: 1.8755, val_loss: 1.9948\n",
            "step 225: train_loss: 1.8800, val_loss: 1.9891\n",
            "step 226: train_loss: 1.8908, val_loss: 1.9938\n",
            "step 227: train_loss: 1.8353, val_loss: 1.9665\n",
            "step 228: train_loss: 1.8413, val_loss: 1.9734\n",
            "step 229: train_loss: 1.8489, val_loss: 1.9953\n",
            "step 230: train_loss: 1.8545, val_loss: 1.9368\n",
            "step 231: train_loss: 1.8518, val_loss: 1.9097\n",
            "step 232: train_loss: 1.8513, val_loss: 1.9681\n",
            "step 233: train_loss: 1.8735, val_loss: 1.9606\n",
            "step 234: train_loss: 1.8734, val_loss: 1.9529\n",
            "step 235: train_loss: 1.8242, val_loss: 1.9627\n",
            "step 236: train_loss: 1.8125, val_loss: 1.9610\n",
            "step 237: train_loss: 1.8044, val_loss: 1.9746\n",
            "step 238: train_loss: 1.8138, val_loss: 1.9385\n",
            "step 239: train_loss: 1.8145, val_loss: 1.9621\n",
            "step 240: train_loss: 1.8249, val_loss: 1.9513\n",
            "step 241: train_loss: 1.8144, val_loss: 1.9729\n",
            "step 242: train_loss: 1.7995, val_loss: 1.9412\n",
            "step 243: train_loss: 1.8159, val_loss: 1.9557\n",
            "step 244: train_loss: 1.8344, val_loss: 1.9369\n",
            "step 245: train_loss: 1.8355, val_loss: 1.9664\n",
            "step 246: train_loss: 1.8027, val_loss: 1.9397\n",
            "step 247: train_loss: 1.7593, val_loss: 1.9417\n",
            "step 248: train_loss: 1.7862, val_loss: 1.9456\n",
            "step 249: train_loss: 1.7380, val_loss: 1.9595\n",
            "step 250: train_loss: 1.8056, val_loss: 1.9284\n",
            "step 251: train_loss: 1.8133, val_loss: 1.8888\n",
            "step 252: train_loss: 1.8030, val_loss: 1.9123\n",
            "step 253: train_loss: 1.7671, val_loss: 1.9569\n",
            "step 254: train_loss: 1.7656, val_loss: 1.9295\n",
            "step 255: train_loss: 1.7893, val_loss: 1.8802\n",
            "step 256: train_loss: 1.7978, val_loss: 1.9503\n",
            "step 257: train_loss: 1.8154, val_loss: 1.9140\n",
            "step 258: train_loss: 1.7908, val_loss: 1.8830\n",
            "step 259: train_loss: 1.7806, val_loss: 1.8935\n",
            "step 260: train_loss: 1.7837, val_loss: 1.8822\n",
            "step 261: train_loss: 1.7629, val_loss: 1.9297\n",
            "step 262: train_loss: 1.7674, val_loss: 1.9223\n",
            "step 263: train_loss: 1.8132, val_loss: 1.8904\n",
            "step 264: train_loss: 1.7431, val_loss: 1.8758\n",
            "step 265: train_loss: 1.7289, val_loss: 1.9031\n",
            "step 266: train_loss: 1.7583, val_loss: 1.9201\n",
            "step 267: train_loss: 1.7652, val_loss: 1.9392\n",
            "step 268: train_loss: 1.7728, val_loss: 1.8957\n",
            "step 269: train_loss: 1.8092, val_loss: 1.8828\n",
            "step 270: train_loss: 1.7501, val_loss: 1.8849\n",
            "step 271: train_loss: 1.7770, val_loss: 1.8831\n",
            "step 272: train_loss: 1.7362, val_loss: 1.9083\n",
            "step 273: train_loss: 1.7600, val_loss: 1.8823\n",
            "step 274: train_loss: 1.7479, val_loss: 1.8781\n",
            "step 275: train_loss: 1.7196, val_loss: 1.8800\n",
            "step 276: train_loss: 1.7544, val_loss: 1.8944\n",
            "step 277: train_loss: 1.7489, val_loss: 1.8687\n",
            "step 278: train_loss: 1.6952, val_loss: 1.8853\n",
            "step 279: train_loss: 1.7291, val_loss: 1.8336\n",
            "step 280: train_loss: 1.7062, val_loss: 1.8990\n",
            "step 281: train_loss: 1.7312, val_loss: 1.8647\n",
            "step 282: train_loss: 1.7246, val_loss: 1.8764\n",
            "step 283: train_loss: 1.7621, val_loss: 1.8454\n",
            "step 284: train_loss: 1.7172, val_loss: 1.8549\n",
            "step 285: train_loss: 1.6956, val_loss: 1.9000\n",
            "step 286: train_loss: 1.7027, val_loss: 1.8601\n",
            "step 287: train_loss: 1.6842, val_loss: 1.8366\n",
            "step 288: train_loss: 1.7590, val_loss: 1.8704\n",
            "step 289: train_loss: 1.7450, val_loss: 1.8508\n",
            "step 290: train_loss: 1.7239, val_loss: 1.8509\n",
            "step 291: train_loss: 1.7540, val_loss: 1.8738\n",
            "step 292: train_loss: 1.7367, val_loss: 1.8586\n",
            "step 293: train_loss: 1.7065, val_loss: 1.8568\n",
            "step 294: train_loss: 1.7091, val_loss: 1.8609\n",
            "step 295: train_loss: 1.7367, val_loss: 1.8255\n",
            "step 296: train_loss: 1.6914, val_loss: 1.8211\n",
            "step 297: train_loss: 1.6710, val_loss: 1.8821\n",
            "step 298: train_loss: 1.7282, val_loss: 1.8161\n",
            "step 299: train_loss: 1.7076, val_loss: 1.8293\n",
            "step 300: train_loss: 1.7247, val_loss: 1.8785\n",
            "step 301: train_loss: 1.6557, val_loss: 1.8367\n",
            "step 302: train_loss: 1.6920, val_loss: 1.8230\n",
            "step 303: train_loss: 1.7045, val_loss: 1.8678\n",
            "step 304: train_loss: 1.7189, val_loss: 1.8313\n",
            "step 305: train_loss: 1.6997, val_loss: 1.8057\n",
            "step 306: train_loss: 1.6952, val_loss: 1.8154\n",
            "step 307: train_loss: 1.6768, val_loss: 1.8120\n",
            "step 308: train_loss: 1.6880, val_loss: 1.8446\n",
            "step 309: train_loss: 1.6800, val_loss: 1.8349\n",
            "step 310: train_loss: 1.6598, val_loss: 1.8280\n",
            "step 311: train_loss: 1.6652, val_loss: 1.8522\n",
            "step 312: train_loss: 1.6769, val_loss: 1.8194\n",
            "step 313: train_loss: 1.6314, val_loss: 1.8238\n",
            "step 314: train_loss: 1.6384, val_loss: 1.7951\n",
            "step 315: train_loss: 1.6376, val_loss: 1.8197\n",
            "step 316: train_loss: 1.6934, val_loss: 1.7921\n",
            "step 317: train_loss: 1.6632, val_loss: 1.8072\n",
            "step 318: train_loss: 1.6869, val_loss: 1.8052\n",
            "step 319: train_loss: 1.6923, val_loss: 1.8027\n",
            "step 320: train_loss: 1.6810, val_loss: 1.8308\n",
            "step 321: train_loss: 1.6935, val_loss: 1.8036\n",
            "step 322: train_loss: 1.6907, val_loss: 1.7939\n",
            "step 323: train_loss: 1.6463, val_loss: 1.8416\n",
            "step 324: train_loss: 1.6590, val_loss: 1.7937\n",
            "step 325: train_loss: 1.6209, val_loss: 1.7729\n",
            "step 326: train_loss: 1.6338, val_loss: 1.7759\n",
            "step 327: train_loss: 1.6893, val_loss: 1.8023\n",
            "step 328: train_loss: 1.6402, val_loss: 1.8053\n",
            "step 329: train_loss: 1.6474, val_loss: 1.8142\n",
            "step 330: train_loss: 1.6558, val_loss: 1.7829\n",
            "step 331: train_loss: 1.6723, val_loss: 1.8064\n",
            "step 332: train_loss: 1.6575, val_loss: 1.7988\n",
            "step 333: train_loss: 1.6171, val_loss: 1.7580\n",
            "step 334: train_loss: 1.6281, val_loss: 1.7857\n",
            "step 335: train_loss: 1.6700, val_loss: 1.8054\n",
            "step 336: train_loss: 1.6260, val_loss: 1.8266\n",
            "step 337: train_loss: 1.6288, val_loss: 1.7783\n",
            "step 338: train_loss: 1.6316, val_loss: 1.7799\n",
            "step 339: train_loss: 1.6273, val_loss: 1.7922\n",
            "step 340: train_loss: 1.6360, val_loss: 1.7910\n",
            "step 341: train_loss: 1.6496, val_loss: 1.7868\n",
            "step 342: train_loss: 1.6356, val_loss: 1.7714\n",
            "step 343: train_loss: 1.5894, val_loss: 1.8036\n",
            "step 344: train_loss: 1.6115, val_loss: 1.7944\n",
            "step 345: train_loss: 1.6285, val_loss: 1.7877\n",
            "step 346: train_loss: 1.6488, val_loss: 1.7717\n",
            "step 347: train_loss: 1.6209, val_loss: 1.7997\n",
            "step 348: train_loss: 1.5982, val_loss: 1.7855\n",
            "step 349: train_loss: 1.6057, val_loss: 1.8144\n",
            "step 350: train_loss: 1.6322, val_loss: 1.7863\n",
            "step 351: train_loss: 1.6456, val_loss: 1.7834\n",
            "step 352: train_loss: 1.6132, val_loss: 1.7607\n",
            "step 353: train_loss: 1.6141, val_loss: 1.7596\n",
            "step 354: train_loss: 1.6194, val_loss: 1.8165\n",
            "step 355: train_loss: 1.6166, val_loss: 1.7642\n",
            "step 356: train_loss: 1.6024, val_loss: 1.7772\n",
            "step 357: train_loss: 1.6156, val_loss: 1.7447\n",
            "step 358: train_loss: 1.6139, val_loss: 1.7838\n",
            "step 359: train_loss: 1.6008, val_loss: 1.7778\n",
            "step 360: train_loss: 1.6192, val_loss: 1.7564\n",
            "step 361: train_loss: 1.5908, val_loss: 1.7353\n",
            "step 362: train_loss: 1.5979, val_loss: 1.7794\n",
            "step 363: train_loss: 1.6025, val_loss: 1.7513\n",
            "step 364: train_loss: 1.6088, val_loss: 1.7852\n",
            "step 365: train_loss: 1.6068, val_loss: 1.7469\n",
            "step 366: train_loss: 1.6308, val_loss: 1.7712\n",
            "step 367: train_loss: 1.6248, val_loss: 1.7623\n",
            "step 368: train_loss: 1.6007, val_loss: 1.7625\n",
            "step 369: train_loss: 1.5557, val_loss: 1.7667\n",
            "step 370: train_loss: 1.6130, val_loss: 1.7508\n",
            "step 371: train_loss: 1.5691, val_loss: 1.7738\n",
            "step 372: train_loss: 1.5481, val_loss: 1.7495\n",
            "step 373: train_loss: 1.6139, val_loss: 1.7301\n",
            "step 374: train_loss: 1.5714, val_loss: 1.7646\n",
            "step 375: train_loss: 1.6070, val_loss: 1.7555\n",
            "step 376: train_loss: 1.5747, val_loss: 1.7285\n",
            "step 377: train_loss: 1.5607, val_loss: 1.7274\n",
            "step 378: train_loss: 1.5967, val_loss: 1.7501\n",
            "step 379: train_loss: 1.5632, val_loss: 1.7485\n",
            "step 380: train_loss: 1.6111, val_loss: 1.7766\n",
            "step 381: train_loss: 1.5480, val_loss: 1.7137\n",
            "step 382: train_loss: 1.5893, val_loss: 1.7998\n",
            "step 383: train_loss: 1.5898, val_loss: 1.7323\n",
            "step 384: train_loss: 1.5970, val_loss: 1.7259\n",
            "step 385: train_loss: 1.6039, val_loss: 1.7222\n",
            "step 386: train_loss: 1.5934, val_loss: 1.7532\n",
            "step 387: train_loss: 1.5581, val_loss: 1.7112\n",
            "step 388: train_loss: 1.5377, val_loss: 1.7193\n",
            "step 389: train_loss: 1.5597, val_loss: 1.6990\n",
            "step 390: train_loss: 1.5701, val_loss: 1.6913\n",
            "step 391: train_loss: 1.5479, val_loss: 1.7200\n",
            "step 392: train_loss: 1.5480, val_loss: 1.7160\n",
            "step 393: train_loss: 1.5460, val_loss: 1.6857\n",
            "step 394: train_loss: 1.5309, val_loss: 1.7192\n",
            "step 395: train_loss: 1.5531, val_loss: 1.7473\n",
            "step 396: train_loss: 1.5197, val_loss: 1.7675\n",
            "step 397: train_loss: 1.5375, val_loss: 1.7015\n",
            "step 398: train_loss: 1.5632, val_loss: 1.7028\n",
            "step 399: train_loss: 1.5355, val_loss: 1.7011\n",
            "step 400: train_loss: 1.5186, val_loss: 1.7370\n",
            "step 401: train_loss: 1.5647, val_loss: 1.7214\n",
            "step 402: train_loss: 1.5112, val_loss: 1.7016\n",
            "step 403: train_loss: 1.5652, val_loss: 1.7387\n",
            "step 404: train_loss: 1.5331, val_loss: 1.7222\n",
            "step 405: train_loss: 1.5559, val_loss: 1.7175\n",
            "step 406: train_loss: 1.5408, val_loss: 1.7327\n",
            "step 407: train_loss: 1.5562, val_loss: 1.6935\n",
            "step 408: train_loss: 1.5245, val_loss: 1.6970\n",
            "step 409: train_loss: 1.5434, val_loss: 1.6921\n",
            "step 410: train_loss: 1.5554, val_loss: 1.6861\n",
            "step 411: train_loss: 1.4963, val_loss: 1.6937\n",
            "step 412: train_loss: 1.5024, val_loss: 1.6934\n",
            "step 413: train_loss: 1.4990, val_loss: 1.7147\n",
            "step 414: train_loss: 1.5314, val_loss: 1.6974\n",
            "step 415: train_loss: 1.5076, val_loss: 1.7043\n",
            "step 416: train_loss: 1.5387, val_loss: 1.6930\n",
            "step 417: train_loss: 1.5033, val_loss: 1.7210\n",
            "step 418: train_loss: 1.5101, val_loss: 1.7394\n",
            "step 419: train_loss: 1.5287, val_loss: 1.7584\n",
            "step 420: train_loss: 1.4872, val_loss: 1.7323\n",
            "step 421: train_loss: 1.5321, val_loss: 1.6902\n",
            "step 422: train_loss: 1.5488, val_loss: 1.6910\n",
            "step 423: train_loss: 1.5058, val_loss: 1.6921\n",
            "step 424: train_loss: 1.5359, val_loss: 1.7358\n",
            "step 425: train_loss: 1.5211, val_loss: 1.6914\n",
            "step 426: train_loss: 1.5025, val_loss: 1.7009\n",
            "step 427: train_loss: 1.5193, val_loss: 1.6966\n",
            "step 428: train_loss: 1.5045, val_loss: 1.6987\n",
            "step 429: train_loss: 1.5118, val_loss: 1.7112\n",
            "step 430: train_loss: 1.5337, val_loss: 1.7048\n",
            "step 431: train_loss: 1.5040, val_loss: 1.7090\n",
            "step 432: train_loss: 1.5633, val_loss: 1.7297\n",
            "step 433: train_loss: 1.5214, val_loss: 1.7058\n",
            "step 434: train_loss: 1.5302, val_loss: 1.6812\n",
            "step 435: train_loss: 1.4966, val_loss: 1.6659\n",
            "step 436: train_loss: 1.5121, val_loss: 1.6590\n",
            "step 437: train_loss: 1.4906, val_loss: 1.6766\n",
            "step 438: train_loss: 1.4897, val_loss: 1.6915\n",
            "step 439: train_loss: 1.5182, val_loss: 1.7267\n",
            "step 440: train_loss: 1.4889, val_loss: 1.6842\n",
            "step 441: train_loss: 1.5347, val_loss: 1.6547\n",
            "step 442: train_loss: 1.4881, val_loss: 1.7083\n",
            "step 443: train_loss: 1.4982, val_loss: 1.6636\n",
            "step 444: train_loss: 1.4765, val_loss: 1.6769\n",
            "step 445: train_loss: 1.4961, val_loss: 1.7162\n",
            "step 446: train_loss: 1.5169, val_loss: 1.6841\n",
            "step 447: train_loss: 1.5246, val_loss: 1.7169\n",
            "step 448: train_loss: 1.4981, val_loss: 1.6948\n",
            "step 449: train_loss: 1.4516, val_loss: 1.6676\n",
            "step 450: train_loss: 1.4833, val_loss: 1.6857\n",
            "step 451: train_loss: 1.5319, val_loss: 1.7064\n",
            "step 452: train_loss: 1.5020, val_loss: 1.6821\n",
            "step 453: train_loss: 1.4520, val_loss: 1.6856\n",
            "step 454: train_loss: 1.4688, val_loss: 1.7044\n",
            "step 455: train_loss: 1.4766, val_loss: 1.7194\n",
            "step 456: train_loss: 1.4585, val_loss: 1.7195\n",
            "step 457: train_loss: 1.4958, val_loss: 1.6734\n",
            "step 458: train_loss: 1.4705, val_loss: 1.6724\n",
            "step 459: train_loss: 1.4682, val_loss: 1.6773\n",
            "step 460: train_loss: 1.5069, val_loss: 1.6850\n",
            "step 461: train_loss: 1.5252, val_loss: 1.6921\n",
            "step 462: train_loss: 1.4980, val_loss: 1.6795\n",
            "step 463: train_loss: 1.4925, val_loss: 1.6427\n",
            "step 464: train_loss: 1.5003, val_loss: 1.6849\n",
            "step 465: train_loss: 1.4837, val_loss: 1.6460\n",
            "step 466: train_loss: 1.4699, val_loss: 1.6663\n",
            "step 467: train_loss: 1.4693, val_loss: 1.6679\n",
            "step 468: train_loss: 1.4833, val_loss: 1.6494\n",
            "step 469: train_loss: 1.4964, val_loss: 1.7177\n",
            "step 470: train_loss: 1.4634, val_loss: 1.6652\n",
            "step 471: train_loss: 1.4509, val_loss: 1.6048\n",
            "step 472: train_loss: 1.4696, val_loss: 1.6767\n",
            "step 473: train_loss: 1.4766, val_loss: 1.6562\n",
            "step 474: train_loss: 1.4671, val_loss: 1.6376\n",
            "step 475: train_loss: 1.4567, val_loss: 1.6646\n",
            "step 476: train_loss: 1.4723, val_loss: 1.6807\n",
            "step 477: train_loss: 1.4763, val_loss: 1.6392\n",
            "step 478: train_loss: 1.4431, val_loss: 1.6416\n",
            "step 479: train_loss: 1.4385, val_loss: 1.6588\n",
            "step 480: train_loss: 1.4566, val_loss: 1.6422\n",
            "step 481: train_loss: 1.5017, val_loss: 1.6517\n",
            "step 482: train_loss: 1.4809, val_loss: 1.6697\n",
            "step 483: train_loss: 1.4273, val_loss: 1.5841\n",
            "step 484: train_loss: 1.4470, val_loss: 1.6575\n",
            "step 485: train_loss: 1.4151, val_loss: 1.7019\n",
            "step 486: train_loss: 1.4835, val_loss: 1.6736\n",
            "step 487: train_loss: 1.4485, val_loss: 1.6897\n",
            "step 488: train_loss: 1.4431, val_loss: 1.6629\n",
            "step 489: train_loss: 1.4303, val_loss: 1.6349\n",
            "step 490: train_loss: 1.4435, val_loss: 1.6616\n",
            "step 491: train_loss: 1.4223, val_loss: 1.6324\n",
            "step 492: train_loss: 1.4483, val_loss: 1.6442\n",
            "step 493: train_loss: 1.4144, val_loss: 1.6729\n",
            "step 494: train_loss: 1.4498, val_loss: 1.6852\n",
            "step 495: train_loss: 1.4639, val_loss: 1.5960\n",
            "step 496: train_loss: 1.4554, val_loss: 1.6640\n",
            "step 497: train_loss: 1.4739, val_loss: 1.6851\n",
            "step 498: train_loss: 1.4511, val_loss: 1.6382\n",
            "step 499: train_loss: 1.4631, val_loss: 1.6450\n",
            "1.5073719024658203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device='cuda')\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "id": "wZxWe2Nw4Mv9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df7e968-c25c-45a5-933e-13fb7b3933b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "when the so\n",
            "theselr, horry as pached with them, in shas for heer, and then tine\n",
            "Sorce Wizard obe darhel agren. It Is\n",
            "Carchunger vegetey after his headfers fownor-jothings, \"becan one with,\n",
            "the ming to yethou ne. \n",
            "\"We it a dimplect toodI'd sme,\" voitary; so the raitter a very spight.\n",
            "\n",
            "\"But Iffurery too Mabook flacim One,\" and speecled in Jim,\" the repided Dorothy.\n",
            "\n",
            "\"Your in wenterere?\" a asked Zeb, and slise we to them,\" can8y \"I\n",
            "gooMy had true you make the alloon all aboroun the Wizard, of Zeb, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Large Language Model"
      ],
      "metadata": {
        "id": "y3GD1kfQGu_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For datasets to use for LLMs, you can use the OpenWebText Corpus which contains highly upvoted reddit posts (~40 GB) Common Crawl is a large scale database on the order of petabytes. A paper for a list of data is called the survey of large language models.\n",
        "\n",
        "These datasets are large so we cannot read 45GB of text into RAM at once, its not feasible. So we have to load data some other way.\n",
        "\n",
        "Install the dataset from the website locally and extract them.\n",
        "\n",
        "\n",
        "As we're reading the little compressed file, we can take the new characters from them and push it onto the vocab file.\n",
        "\n",
        "It will also be more efficient to create an output train file and output val file"
      ],
      "metadata": {
        "id": "4kPmUnDGGxPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import lzma #to handle xz files\n",
        "from tqdm import tqdm #progress bar\n",
        "\n",
        "import mmap\n",
        "import random"
      ],
      "metadata": {
        "id": "sCQgonmYJVhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xz_files_in_dir(directory):\n",
        "  \"\"\"Takes directory as input and returns all xz files within the directory\"\"\"\n",
        "\n",
        "  files = []\n",
        "  for filename in os.list(directory):\n",
        "    if filename.endswith(\".xz\") and os.path.isfile(os.path.join(directory, filename)):\n",
        "      #check if it is a file and not a directory or a link\n",
        "      files.append(filename)\n",
        "  return files\n",
        "\n",
        "folder_path = #The folder wherever the xz file is located\n",
        "output_file_train = \"output_train.txt\" #The output file\n",
        "output_file_val = \"output_val.txt\"\n",
        "vocab_file = \"vocab.txt\" #vocabulary file\n",
        "\n",
        "files = xz_files_in_dir(folder_path)\n",
        "total_files = len(files)\n",
        "\n",
        "split_index = int(total_files * 0.9)\n",
        "files_train = files[:split_index]\n",
        "files_val = files[split_index:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "eEFMZZThJclB",
        "outputId": "f41b5b8a-35af-4709-b8a4-51d8df0210e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-47-34a2174b2893>, line 12)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-47-34a2174b2893>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    folder_path = #The folder wherever the xz file is located\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set()\n",
        "\n",
        "\n",
        "with open(output_file_train, \"w\", encoding=\"utf-8\") as outfile:\n",
        "  for filename in tqdm(files_train, total=len(files_train)):\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    with lzma.open(file_path, \"rt\", encoding=\"utf-8\") as infile:\n",
        "      text = infile.read()\n",
        "      outfile.write(text)\n",
        "      characters = set(text)\n",
        "      vocab.update(characters)\n",
        "\n",
        "with open(output_file_val, \"w\", encoding=\"utf-8\") as outfile:\n",
        "  for filename in tqdm(files_val, total=len(files_val)):\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    with lzma.open(file_path, \"rt\", encoding=\"utf-8\") as infile:\n",
        "      text = infile.read()\n",
        "      outfile.write(text)\n",
        "      characters = set(text)\n",
        "      vocab.update(characters)\n",
        "\n",
        "with open(vocab_file, \"w\", encoding=\"utf-8\") as vfile:\n",
        "  for char in vocab:\n",
        "    vfile.write(char)\n",
        "    vfile.write(\"\\n\")"
      ],
      "metadata": {
        "id": "a8QyRp7xLSzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to change the way we load our file and get batches"
      ],
      "metadata": {
        "id": "YXY0-Mq-PCAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = \"\"\n",
        "\n",
        "with open(\"vocab.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "  chars = sorted(list(set(text)))\n",
        "\n",
        "vocab_size = len(chars)\n",
        "\n",
        "string_to_int = {ch:i for i, ch in enumerate(chars)} #Create mapping\n",
        "int_to_string = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda x: [string_to_int[c] for c in x]\n",
        "decode = lambda x: \"\".join([int_to_string[i] for i in x])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "bPJZEvewPGEp",
        "outputId": "90c63728-0388-4080-ef89-0367812e75e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'vocab.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-7362df4cdac4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mchars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vocab.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mchars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vocab.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "  data = get_random_chunk(split) #get from our file\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device) #load data on the gpu\n",
        "  return x, y\n",
        "\n",
        "#Get files by memory mapping by looking only at pieces of file in very large files without opening whole thing\n",
        "def get_random_chunk(split):\n",
        "  filename = \"output_train.txt\" if split == \"train\" else \"output_val.txt\"\n",
        "\n",
        "  with open(filename, 'rb') as f:\n",
        "    with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
        "      file_size = len(mm)\n",
        "      start_pos = random.randint(0, (file_size) - block_size * batch_size)\n",
        "\n",
        "      mm.seek(start_pos)\n",
        "      block = mm.read(block_size * batch_size - 1)\n",
        "\n",
        "      #convert binary to utf-8 and ignore the erroneous data\n",
        "      decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
        "      data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "uU2tIW5DPXjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then run model like normal"
      ],
      "metadata": {
        "id": "9NFqjl1nRdcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving the model**"
      ],
      "metadata": {
        "id": "8RHb3aqhh8a-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use torch.load and torch.save to save our model parameters and architecture (its the GPT Language Model class).\n",
        "\n",
        "You can use pickle as well, it only works on one GPU (see up). Then we can train a bit of our model, take a break and then train again."
      ],
      "metadata": {
        "id": "pWdzjohQh-9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading model\")\n",
        "\n",
        "with open('model-01.pkl', 'rb') as f:\n",
        "  model = pickle.load(f)\n",
        "\n",
        "print(\"Loaded successfully\")\n",
        "\n",
        "m = model.to(device)\n",
        "#and then run the training loop being careful to hashtag out\n",
        "#model = GPTLanguageModel because we dont want to create the model again"
      ],
      "metadata": {
        "id": "5oAQbx0nh-Wx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}